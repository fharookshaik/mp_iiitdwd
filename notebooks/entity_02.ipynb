{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLBkWDLqrlnk",
    "outputId": "07e457f4-84d5-4943-ba68-2c78134bc1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oSA8gnxxrvBB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGoO9ccizYFz",
    "outputId": "885d0d08-97e8-4d82-be80-af9966295c40"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gIoLm77EsJjS"
   },
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "DATASET_PATH = Path('D:\\fharookshaik\\major_project\\dataset')\n",
    "TRAIN_IMAGES_DIR_PATH = os.path.join(DATASET_PATH,'Train','images')\n",
    "\n",
    "TRAIN_CSV_PATH = os.path.join(DATASET_PATH,'Train','train.csv')\n",
    "VALIDATE_CSV_PATH = os.path.join(DATASET_PATH,'Train','val.csv')\n",
    "\n",
    "TEST_IMAGES_DIR_PATH = os.path.join(DATASET_PATH,'Test','images')\n",
    "TEST_CSV_PATH = os.path.join(DATASET_PATH,'Test','test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wtGvtqBpsQS5"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'D:\\x0charookshaik\\\\major_project\\\\dataset\\\\Test\\\\test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\fharookshaik\\major_project\\notebooks\\entity_02.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/fharookshaik/major_project/notebooks/entity_02.ipynb#ch0000004?line=0'>1</a>\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(TEST_CSV_PATH)\n",
      "File \u001b[1;32md:\\fharookshaik\\major_project\\env\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\fharookshaik\\major_project\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\fharookshaik\\major_project\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=571'>572</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=573'>574</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=574'>575</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=576'>577</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\fharookshaik\\major_project\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=929'>930</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=931'>932</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=932'>933</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32md:\\fharookshaik\\major_project\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1212'>1213</a>\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1213'>1214</a>\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1214'>1215</a>\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1215'>1216</a>\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1216'>1217</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1217'>1218</a>\u001b[0m     f,\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1218'>1219</a>\u001b[0m     mode,\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1219'>1220</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1220'>1221</a>\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1221'>1222</a>\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1222'>1223</a>\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1223'>1224</a>\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1224'>1225</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1225'>1226</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1226'>1227</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/parsers/readers.py?line=1227'>1228</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\fharookshaik\\major_project\\env\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=783'>784</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=784'>785</a>\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=785'>786</a>\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=786'>787</a>\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=787'>788</a>\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=788'>789</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=789'>790</a>\u001b[0m             handle,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=790'>791</a>\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=791'>792</a>\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=792'>793</a>\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=793'>794</a>\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=794'>795</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=795'>796</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=796'>797</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/fharookshaik/major_project/env/lib/site-packages/pandas/io/common.py?line=797'>798</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'D:\\x0charookshaik\\\\major_project\\\\dataset\\\\Test\\\\test.csv'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TEST_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LceGN2G7R-E4",
    "outputId": "9b9e41ae-21ad-482b-b6e0-7cdbf6dc72d8"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rDn7NqR0R94A",
    "outputId": "9544a099-65d8-4b30-9725-6a165fd6e390"
   },
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMxV9VDPSAv4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "GKEPw9_wsS6r",
    "outputId": "988018da-cf0d-428d-de8e-4d44ec2c8d02"
   },
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "train_df['OCR'] = train_df['OCR'].fillna(\"\")\n",
    "train_df['entity_list'] = train_df['entity_list'].fillna({i: [] for i in train_df.index})\n",
    "\n",
    "# train_df['hero'] = train_df['hero'].fillna({i: [] for i in train_df.index})\n",
    "# train_df['villain'] = train_df['villain'].fillna({i: [] for i in train_df.index})\n",
    "# train_df['victim'] = train_df['victim'].fillna({i: [] for i in train_df.index})\n",
    "# train_df['other'] = train_df['other'].fillna({i: [] for i in train_df.index})\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OvGNZYQi3ESq",
    "outputId": "480ee338-4666-487e-bb1c-27aeef782d82"
   },
   "outputs": [],
   "source": [
    "train_df['OCR'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcRJyWLzHJi5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bz-FpLKjsWZ0",
    "outputId": "b6de18fa-8df8-4861-ec8e-933b759b0b5f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ucz_5ClsbSI"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def sentences_to_nouns(sentence):\n",
    "    # case normalization\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Remove unwanted chracters\n",
    "    sentence = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", sentence)\n",
    "\n",
    "    # Remove Stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in (stop)])\n",
    "\n",
    "    # Tokenize sentence\n",
    "    list_tokns = sentence.split()\n",
    "\n",
    "    # Find nouns from sentence\n",
    "    pos = nltk.pos_tag(list_tokns)\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # do the nlp stuff\n",
    "    nouns = [word for (word,pos) in nltk.pos_tag(list_tokns) if is_noun(pos)]\n",
    "\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities = []\n",
    "# for idx,val in train_df.iterrows():\n",
    "#     entities.append(sentences_to_nouns(val.get('OCR')))\n",
    "\n",
    "# print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SLGDMvdseGh"
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "for idx,val in train_df.iterrows():\n",
    "    entities.append([word.strip('\\'') for word in train_df['entity_list'][idx].strip(\"][\").split(\", \")])\n",
    "\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMTDiObG0JCr",
    "outputId": "d1f9eab2-abf3-4060-ce96-54079cf96c24"
   },
   "outputs": [],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zrHNJWIsgvf",
    "outputId": "4fd5b1da-5b35-4cb0-8111-848b1538b303"
   },
   "outputs": [],
   "source": [
    "entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLTAiM5r0xhe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "SlT31bfbsnk7",
    "outputId": "f974d26f-2b27-4696-9106-609dfc42c9c8"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## Below code runs for approx 01:30 hrs\n",
    "#########################################\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Find sentences according to entities\n",
    "enty_sent_dict = {}\n",
    "\n",
    "# for entity in enty_sent_dict:\n",
    "#     enty_sent_dict[entity] = []\n",
    "\n",
    "def find_enty_sent(entity):\n",
    "    temp_sent_list = []\n",
    "    for idx,val in train_df.iterrows():\n",
    "        sentence = val.get('OCR').lower()\n",
    "        sentence = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", sentence)\n",
    "        if re.search(entity,sentence):\n",
    "            temp_sent_list.append(sentence)\n",
    "    enty_sent_dict[entity] = temp_sent_list\n",
    "    return True\n",
    "\n",
    "def run():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        result = list(tqdm(executor.map(find_enty_sent,unique_entities),total=len(unique_entities)))\n",
    "    return result\n",
    "    # for idx,res in enumerate(result):\n",
    "    #     print(f'{idx} : {res}')\n",
    "run()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLhwAIeaAiRZ"
   },
   "outputs": [],
   "source": [
    "# enty_sent_dict = json.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrVgLwP4ttsG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5HXpKtS9Gyc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVPuslqcR1Sw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNJ-I_5o9Lye",
    "outputId": "b88d5c88-a445-4123-dc1f-3883b14bc3d1"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for idx,val in train_df.iterrows():\n",
    "    text = val.get('OCR').lower()\n",
    "    text = text.split(\".\")\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULfNGxAU9NLU",
    "outputId": "e16ddd8d-be4a-48ed-9a6b-70051e9ff9ae"
   },
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyKVpGyYLBnv",
    "outputId": "eb1bf340-702d-4fa4-da09-ce3ed93123da"
   },
   "outputs": [],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FE4va7I9Rlp",
    "outputId": "405ef255-4d86-485a-afac-1602f41ee80f"
   },
   "outputs": [],
   "source": [
    "print(sentences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bETD-9nO9TMs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3qVHIPZ9Vf_",
    "outputId": "65365092-88b2-4605-e709-7016919125be"
   },
   "outputs": [],
   "source": [
    "for i,entity in enumerate(entities):\n",
    "  print(i)\n",
    "  print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax_pxCu6M-e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i241eP1m9XB5",
    "outputId": "4c201928-25c7-45f5-be65-183d0d48c2ce"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "enty_sent_dict = {}\n",
    "\n",
    "for i,entity in enumerate(entities[0:2]):\n",
    "  print(i)\n",
    "  per_meme_dict = {}\n",
    "  for entity_per_meme in entity:\n",
    "      print(\"entity = \"+ entity_per_meme)\n",
    "      temp_sent_list = []\n",
    "      for sentence in sentences[i]:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", sentence)\n",
    "        words = sentence.split(\" \")\n",
    "        #print(words)\n",
    "        if entity_per_meme in words:\n",
    "          print( \"Sentence = \" + sentence)\n",
    "\n",
    "          entity_index = words.index(entity_per_meme)\n",
    "          print(\"Index = \" + str(entity_index))\n",
    "\n",
    "          sentence_01 = words[entity_index-2:entity_index]\n",
    "          sentence_02 = words[entity_index:entity_index+4]\n",
    "          sentence_ = sentence_01 + sentence_02\n",
    "          sentence_ = ' '.join(sentence_)\n",
    "          print(\"Sentence_ = \"+str(sentence_))\n",
    "\n",
    "          temp_sent_list.append(sentence_)\n",
    "\n",
    "      per_meme_dict[entity_per_meme] = temp_sent_list\n",
    "  print(per_meme_dict)\n",
    "  enty_sent_dict[train_df['image'][i]] = per_meme_dict\n",
    "\n",
    "print(\"\\n Whole dictionary = \")\n",
    "print(enty_sent_dict)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yR4zBaVRpFQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fccK4j7UNzJH"
   },
   "outputs": [],
   "source": [
    "\"\"\" For whole data \"\"\"\n",
    "\n",
    "\n",
    "enty_sent_dict = {}\n",
    "\n",
    "for i,entity in enumerate(entities):\n",
    "  #print(i)\n",
    "  per_meme_dict = {}\n",
    "  for entity_per_meme in entity:\n",
    "      #print(\"entity = \"+ entity_per_meme)\n",
    "      temp_sent_list = []\n",
    "      for sentence in sentences[i]:\n",
    "        if sentence.find(entity_per_meme):\n",
    "          temp_sent_list.append(sentence)\n",
    "        # if re.search(entity_per_meme,sentence):\n",
    "        #   #print(sentence + \". \\n\")\n",
    "        #   temp_sent_list.append(sentence)\n",
    "\n",
    "      per_meme_dict[entity_per_meme] = temp_sent_list\n",
    "  #print(per_meme_dict)\n",
    "  enty_sent_dict[train_df['image'][i]] = per_meme_dict\n",
    "\n",
    "#print(\"\\n Whole dictionary = \")\n",
    "#print(enty_sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-NV4UuJNyzG",
    "outputId": "7727e2ea-11fc-470d-a867-5f17f0a88dca"
   },
   "outputs": [],
   "source": [
    "print(\"Whole dictionary = \")\n",
    "print(enty_sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9TGObg8RmCg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUsFzbi25pOO"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28zQYN7htTF1"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_PATH,'enty_sent_linking.json'),'w') as f:\n",
    "    json.dump(enty_sent_dict,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUXXy0G__Qoo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlaab5ngQxlw",
    "outputId": "5d233a57-d717-415c-b3e3-b6cb94573831"
   },
   "outputs": [],
   "source": [
    "print(\"length of dictionary = \", len(enty_sent_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fad5u3YIRi4S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlMZv4pv09qx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxpZObV61vME"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igkf3kLXzunn",
    "outputId": "a9c72a1a-b8e8-4160-88a0-8fdf04df3753"
   },
   "outputs": [],
   "source": [
    "ENTITY_SENT_LINKED = os.path.join(DATASET_PATH,'enty_sent_linking.json')\n",
    "ENTITY_SENT_LINKED_FILE = open(ENTITY_SENT_LINKED)\n",
    "# DataFrame\n",
    "# entity_sent_df = pd.read_json(ENTITY_SENT_LINKED_FILE)\n",
    "\n",
    "data = json.load(ENTITY_SENT_LINKED_FILE)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFtHfh_EAkuL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq3it7VU8uDP"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qme41zbe8u7g"
   },
   "outputs": [],
   "source": [
    "# function to print sentiments of the sentence.\n",
    "def sentiment_scores(sentence):\n",
    " \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "     \n",
    "    #print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "    \"\"\"\n",
    " \n",
    "    #print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    \"\"\"\n",
    "    if sentiment_dict['compound'] >= 0.025 :\n",
    "        print(\"Positive\")\n",
    " \n",
    "    elif sentiment_dict['compound'] >= - 0.05 and sentiment_dict['compound'] <= -0.025:\n",
    "        print(\"Negative + Victim\")\n",
    "\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative + Villain\")\n",
    " \n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    "    \"\"\"\n",
    "\n",
    "    return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcl6RNX6Ag_9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtIVcX131MKp",
    "outputId": "81986ea7-99f2-4805-feb3-9b490403136e"
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for meme_name,ocr in zip(train_df['image'],train_df['OCR']):\n",
    "  #print(meme_name)\n",
    "\n",
    "  per_meme_dict = {\"image\": meme_name,\n",
    "                   \"OCR\": ocr,\n",
    "                   \"hero\": [],\n",
    "                   \"villain\": [],\n",
    "                   \"victim\": [],\n",
    "                   \"other\": []}\n",
    "\n",
    "  meme = data[meme_name]\n",
    "  #print(data[meme_name])\n",
    "  for entity in meme:\n",
    "    #print(\"Entity = \" + entity)\n",
    "    #print(meme[entity])\n",
    "    sentence_list = meme[entity]\n",
    "\n",
    "    sentiment_score_calculated = 0\n",
    "    no_of_sentences = 0\n",
    "    for sentence in sentence_list:\n",
    "      #print(\"Sentence = \" + str(sentence))\n",
    "\n",
    "      sentiment_score_calculated = sentiment_score_calculated + sentiment_scores(sentence)\n",
    "      no_of_sentences = no_of_sentences + 1\n",
    "    try:\n",
    "      sentiment_score_calculated = sentiment_score_calculated / no_of_sentences\n",
    "      #print(sentiment_score_calculated)\n",
    "    except ZeroDivisionError:\n",
    "      sentiment_score_calculated = 0\n",
    "\n",
    "    if sentiment_score_calculated >= 0.05 :\n",
    "      per_meme_dict[\"hero\"].append(entity)\n",
    "    elif sentiment_score_calculated >= - 0.05 and sentiment_score_calculated <= -0.025:\n",
    "      per_meme_dict[\"victim\"].append(entity)\n",
    "    elif sentiment_score_calculated <= - 0.05 :\n",
    "      per_meme_dict[\"villain\"].append(entity)\n",
    "    else :\n",
    "      per_meme_dict[\"other\"].append(entity)\n",
    "\n",
    "  #print(per_meme_dict)\n",
    "  result_dict[meme_name] = per_meme_dict\n",
    "  #print(\"\\n\")\n",
    "\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U5FZB4Y7W1Y",
    "outputId": "92193e3a-1bfc-48a5-8481-8c9126dce49c"
   },
   "outputs": [],
   "source": [
    "print(\"length of dictionary = \", len(result_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjmxDVMn69X_"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_PATH,'result_sentiment_analysis.json'),'w') as f:\n",
    "    json.dump(result_dict,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZhRBo159Ecg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMsHIKHqx2DI"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Filtered final draft of role dictionaries.\n",
    "'''\n",
    "\n",
    "HERO_DICT = {'gentle', 'preserving', 'leadership', 'amazing', 'devoted', 'humble', 'warned', 'surprised', 'humanity', 'brave', 'evacuate', 'redemption', 'smile', 'honor', 'revolutionize', 'leader', 'advocate', 'savior', 'charity', 'sympathies', 'kindness', 'good', 'protect', 'teach', 'reputation', 'respected', 'welfare', 'glory', 'victory', 'winner', 'well', 'contained', 'restoration', 'commitment', 'ability', 'efforts', 'inspire', 'safety', 'allies', 'health', 'strength', 'empowered', 'passion', 'encouraging', 'warm', 'vision', 'scored', 'authorities', 'justice', 'grand', 'admire', 'reshape', 'communities', 'response', 'strengthen', 'bolster', 'intervened', 'motivated', 'reconstruct', 'freedom', 'duty', 'aided', 'conquer', 'smart', 'bravery', 'improve', 'donate', 'wise', 'ingenuity', 'milestone', 'protections', 'expand', 'hero', 'pursuit', 'invent', 'containment', 'achievement', 'supporters'}\n",
    "\n",
    "VILLAIN_DICT = {'contaminate', 'dirty', 'abduct', 'terror', 'worsen', 'crisis', 'lambast', 'abandonment', 'harass', 'subvert', 'virus', 'crime', 'provoke', 'kidnap', 'manipulate', 'alleged', 'refusal', 'trafficking', 'marginalize', 'conformity', 'clampdown', 'villain', 'disparaged', 'cold', 'exacerbate', 'alienate', 'commit', 'trial', 'violence', 'denounced', 'stripped', 'undermine', 'seize', 'persecuted', 'opposing', 'intimidate', 'jailed', 'fool', 'investigation', 'imprisoned', 'bias', 'deception', 'gunshots', 'threaten', 'hoax', 'engulfed', 'blame', 'eruption', 'offensive', 'contempt', 'suggested', 'coercion', 'erase', 'catastrophe', 'rumors', 'weaken', 'pointed', 'treason', 'evil', 'abused', 'sentenced', 'bullet', 'warn', 'devastate', 'convicted', 'rebuke', 'reveal', 'bully', 'collude'}\n",
    "\n",
    "VICTIM_DICT = {'setback', 'injured', 'traumatized', 'prevented', 'healing', 'buried', 'stuck', 'anguished', 'flee', 'suffer', 'casualty', 'trampled', 'forsaken', 'harassed', 'harassment', 'hardship', 'deported', 'howling', 'shocked', 'violence', 'depressed', 'danger', 'mute', 'stripped', 'terrified', 'distrust', 'assassinated', 'shivering', 'sick', 'complain', 'abducted', 'huddled', 'victimized', 'persecuted', 'barricaded', 'devastated', 'kidnapped', 'seized', 'justified', 'evacuated', 'surrendered', 'diagnosed', 'imprisoned', 'independence', 'slave', 'deceased', 'rebuffed', 'target', 'trapped', 'screamed', 'loss', 'trafficked', 'humiliated', 'impairment', 'wounded', 'discriminated', 'disadvantaged', 'blood', 'offended', 'accuses', 'saddens', 'threatened', 'disaster', 'devastation', 'overshadowed', 'tortured', 'abused', 'remonstrated', 'jeopardizing', 'stabbed', 'prey', 'sentenced', 'challenged', 'renounced', 'scared', 'humiliation', 'deaths', 'rescued', 'bleeding'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTlwHF2x1-o8",
    "outputId": "c6b45132-c378-4dc5-d884-344fb3b8a1be"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuCItqKe1YMa"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEG0ssyZ0TN2"
   },
   "outputs": [],
   "source": [
    "def word_similarity(word1, word2):\n",
    "    '''\n",
    "    Returns the Wu-Palmer similarity between the given words.\n",
    "    Values range between 0 (least similar) and 1 (most similar).\n",
    "    '''\n",
    "    syns_w1 = wn.synsets(word1)\n",
    "    syns_w2 = wn.synsets(word2)\n",
    "    score = 0\n",
    "    for w1 in syns_w1:\n",
    "        for w2 in syns_w2:\n",
    "            cur_score = w1.wup_similarity(w2)\n",
    "            cur_score = w2.wup_similarity(w1) if not cur_score else cur_score\n",
    "            if cur_score:\n",
    "                score = max(score, cur_score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdcv_6fcOOMO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yqUVVR0Unnw",
    "outputId": "59bcdcd9-c944-4b10-c7ea-168398a646db"
   },
   "outputs": [],
   "source": [
    "similarity_dict = os.path.join(DATASET_PATH,'similarity_dictionary.json')\n",
    "similarity_dict_file = open(similarity_dict)\n",
    "\n",
    "similarity_data = json.load(similarity_dict_file)\n",
    "print(similarity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJIs8FFHUrOx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQxQxbp3gPC4",
    "outputId": "ce4de106-af66-47f7-fd6d-fbf264f6a7e4"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6oUkE6jhXO3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwJyoz9GqKoz",
    "outputId": "244a6dd0-2d9c-4869-a745-160b1e9ed06d"
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "assignment_dict = {0:\"hero\",\n",
    "                   1:\"villain\",\n",
    "                   2:\"victim\",\n",
    "                   3:\"other\"}\n",
    "\n",
    "for meme_name,ocr in zip(train_df['image'],train_df['OCR']):\n",
    "  #print(meme_name)\n",
    "\n",
    "  per_meme_dict = {\"image\": meme_name,\n",
    "                   \"OCR\": ocr,\n",
    "                   \"hero\": [],\n",
    "                   \"villain\": [],\n",
    "                   \"victim\": [],\n",
    "                   \"other\": []}\n",
    "\n",
    "  meme = data[meme_name]\n",
    "  #print(data[meme_name])\n",
    "\n",
    "  for entity in meme:\n",
    "    #print(\"Entity = \" + entity)\n",
    "    #print(meme[entity])\n",
    "    sentence_list = meme[entity]\n",
    "\n",
    "    similarity_list = [0, 0, 0]\n",
    "    number_of_words = 0\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "      #print(\"Sentence = \" + sentence)\n",
    "      words = word_tokenize(sentence)\n",
    "      #print(words)\n",
    "\n",
    "      for word in words:\n",
    "        number_of_words = number_of_words + 1\n",
    "        #word = ps.stem(word)\n",
    "        #print(\"Word = \" + word)\n",
    "\n",
    "        try:\n",
    "          #print(similarity_data[word])\n",
    "          if similarity_data[word]:\n",
    "            similarity_list[0] = similarity_list[0] + similarity_data[word][0]\n",
    "            similarity_list[1] = similarity_list[1] + similarity_data[word][1]\n",
    "            similarity_list[2] = similarity_list[2] + similarity_data[word][2]\n",
    "        \n",
    "        except:\n",
    "            #print(\"No entry Found\")\n",
    "            #print(\"[0, 0, 0]\")\n",
    "            pass\n",
    "\n",
    "    #similarity_list[0] = similarity_list[0] / number_of_words\n",
    "    #similarity_list[1] = similarity_list[1] / number_of_words\n",
    "    #similarity_list[2] = similarity_list[2] / number_of_words\n",
    "\n",
    "    #print(similarity_list)\n",
    "\n",
    "    max_value = max(similarity_list)\n",
    "    max_index = similarity_list.index(max_value)\n",
    "\n",
    "    #print(\"Max_value = \" + str(max_value))\n",
    "    #print(\"Max_index = \" + str(max_index))\n",
    "\n",
    "    difference = similarity_list[0] - similarity_list[1]\n",
    "    difference = abs(difference) + abs(similarity_list[0] - similarity_list[2])\n",
    "    #print(\"Difference = \" + str(difference))\n",
    "\n",
    "    if difference < (0.5229945):\n",
    "      per_meme_dict['other'].append(entity)\n",
    "\n",
    "    else:\n",
    "      per_meme_dict[assignment_dict[max_index]].append(entity)\n",
    "\n",
    "    #print(\"\\n\")\n",
    "\n",
    "  #print(per_meme_dict)\n",
    "  result_dict[meme_name] = per_meme_dict\n",
    "  #print(\"\\n\")\n",
    "\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJB9IxM3qkC3"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_PATH,'result_similarity_dictionary.json'),'w') as f:\n",
    "    json.dump(result_dict,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmSQnDLihiB0",
    "outputId": "3f04cb44-c07c-4946-b54b-677fc1390f3e"
   },
   "outputs": [],
   "source": [
    "print(len(result_dict))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "entity_02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
